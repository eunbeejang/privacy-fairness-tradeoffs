/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/opacus/privacy_engine.py:110: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.
  warnings.warn(
RegressionModel(
  (embs): ModuleList(
    (0): Embedding(13, 6)
    (1): Embedding(5, 2)
    (2): Embedding(9, 4)
    (3): Embedding(4, 2)
    (4): Embedding(4, 2)
    (5): Embedding(4, 2)
    (6): Embedding(3, 1)
    (7): Embedding(11, 5)
    (8): Embedding(6, 3)
    (9): Embedding(4, 2)
  )
  (lins): ModuleList(
    (0): Linear(in_features=39, out_features=1000, bias=True)
    (1): Linear(in_features=1000, out_features=500, bias=True)
    (2): Linear(in_features=500, out_features=250, bias=True)
  )
  (bns): ModuleList(
    (0): GroupNorm(1, 1000, eps=1e-05, affine=True)
    (1): GroupNorm(1, 500, eps=1e-05, affine=True)
    (2): GroupNorm(1, 250, eps=1e-05, affine=True)
  )
  (outp): Linear(in_features=250, out_features=1, bias=True)
  (emb_drop): Dropout(p=0.04, inplace=False)
  (drops): ModuleList(
    (0): Dropout(p=0.001, inplace=False)
    (1): Dropout(p=0.01, inplace=False)
    (2): Dropout(p=0.01, inplace=False)
  )
  (bn): GroupNorm(1, 10, eps=1e-05, affine=True)
  (activation): Sigmoid()
) 


=== RUN # 0 ====================================

  0%|          | 0/572 [00:00<?, ?it/s]  0%|          | 1/572 [00:00<04:44,  2.00it/s]  0%|          | 2/572 [00:00<04:30,  2.11it/s]  1%|          | 3/572 [00:01<04:18,  2.20it/s]  1%|          | 4/572 [00:01<04:11,  2.26it/s]  1%|          | 5/572 [00:02<04:06,  2.30it/s]  1%|          | 6/572 [00:02<04:04,  2.32it/s]  1%|          | 7/572 [00:02<04:00,  2.35it/s]  1%|▏         | 8/572 [00:03<03:57,  2.38it/s]  2%|▏         | 9/572 [00:03<03:54,  2.40it/s]  2%|▏         | 10/572 [00:04<03:52,  2.42it/s]  2%|▏         | 11/572 [00:04<03:50,  2.43it/s]  2%|▏         | 12/572 [00:05<03:49,  2.44it/s]  2%|▏         | 13/572 [00:05<03:47,  2.46it/s]  2%|▏         | 14/572 [00:05<03:46,  2.46it/s]  3%|▎         | 15/572 [00:06<03:45,  2.47it/s]  3%|▎         | 16/572 [00:06<03:46,  2.46it/s]  3%|▎         | 17/572 [00:07<03:44,  2.47it/s]  3%|▎         | 18/572 [00:07<03:43,  2.47it/s]  3%|▎         | 19/572 [00:07<03:43,  2.47it/s]  3%|▎         | 20/572 [00:08<03:43,  2.47it/s]  4%|▎         | 21/572 [00:08<03:42,  2.48it/s]  4%|▍         | 22/572 [00:09<03:41,  2.48it/s]  4%|▍         | 23/572 [00:09<03:40,  2.49it/s]  4%|▍         | 24/572 [00:09<03:42,  2.46it/s]  4%|▍         | 25/572 [00:10<03:42,  2.46it/s]  5%|▍         | 26/572 [00:10<03:42,  2.46it/s]  5%|▍         | 27/572 [00:11<03:42,  2.45it/s]  5%|▍         | 28/572 [00:11<03:43,  2.43it/s]  5%|▌         | 29/572 [00:11<03:42,  2.44it/s]  5%|▌         | 30/572 [00:12<03:42,  2.44it/s]  5%|▌         | 31/572 [00:12<03:41,  2.44it/s]  6%|▌         | 32/572 [00:13<03:42,  2.43it/s]  6%|▌         | 33/572 [00:13<03:41,  2.43it/s]  6%|▌         | 34/572 [00:13<03:40,  2.44it/s]  6%|▌         | 35/572 [00:14<03:40,  2.43it/s]  6%|▋         | 36/572 [00:14<03:41,  2.42it/s]  6%|▋         | 37/572 [00:15<03:41,  2.42it/s]  7%|▋         | 38/572 [00:15<03:40,  2.43it/s]  7%|▋         | 39/572 [00:16<03:39,  2.43it/s]  7%|▋         | 40/572 [00:16<03:40,  2.42it/s]  7%|▋         | 41/572 [00:16<03:39,  2.42it/s]  7%|▋         | 42/572 [00:17<03:37,  2.44it/s]  7%|▋         | 42/572 [00:17<03:42,  2.38it/s]
Traceback (most recent call last):
  File "main.py", line 222, in <module>
    main()
  File "main.py", line 170, in main
    train(args, model, device, train_data, optimizer, epoch)
  File "/Users/eunbeejang/Desktop/thesis/privacy-fairness-tradeoffs/run_train.py", line 30, in train
    optimizer.step()
  File "/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/opacus/privacy_engine.py", line 190, in dp_step
    self.privacy_engine.step()
  File "/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/opacus/privacy_engine.py", line 260, in step
    self.clipper.clip_and_accumulate()
  File "/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/opacus/per_sample_gradient_clip.py", line 211, in clip_and_accumulate
    summed_grad = self._weighted_sum(
  File "/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/opacus/per_sample_gradient_clip.py", line 320, in _weighted_sum
    return torch.einsum("i,i...", batch_weight, param)
  File "/Users/eunbeejang/miniconda3/envs/py38/lib/python3.8/site-packages/torch/functional.py", line 327, in einsum
    return _VF.einsum(equation, operands)
KeyboardInterrupt
